{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "727b6ac3",
   "metadata": {},
   "source": [
    "# Twin-Delayed Deep Deterministic Policy Gradients\n",
    "\n",
    "Authors: Eshan Savla, Raphael Aberle, Leo Schäfer\n",
    "\n",
    "<br>\n",
    "This Jupyter Notebook gives an overview to the Twin-Delayed Deep Deterministic Policy Gradients (TD3)....\n",
    "\n",
    "- ToDo: Add Motivation \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Table of contents**:\n",
    "\n",
    "   - Environment description - Ant-v3   ---> ?\n",
    "   - DDPG - brief summary   ---> ?\n",
    "\n",
    "1. Differences TD3 to DDPG <br>\n",
    "   1.1. Clipped double Q-Learning <br>\n",
    "   1.2. Delayed policy update <br>\n",
    "   1.3. Target policy smooting\n",
    "2. Our TD3 implementation <br>\n",
    "   2.1. RL algorithmus extension <br>\n",
    "   2.2. Hyperparameters <br>\n",
    "   2.3. Training <br>\n",
    "   2.4. Enjoy \n",
    "3. Implementation tests\n",
    "4. Benchmark - Discussion <br>\n",
    "   4.1. Our TD3 vs DDPG <br>\n",
    "   4.2. Our TD3 vs Stable Baselines\n",
    "5. Summary \n",
    "6. Conclusion \n",
    "7. Further material\n",
    "\n",
    "<br> \n",
    "<p>\n",
    "\n",
    "Info: \n",
    "- Add mathematical equations from spinning up in the markdown and then the implementation (reference to code lines)\n",
    "\n",
    "- - - "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "31283171-0274-4a4e-9a04-527cc6079400",
   "metadata": {},
   "source": [
    "## **1. Differences DDPG and TD3**\n",
    "\n",
    "TD3 is a direct successor of DDPG and improves it using three major extensions.\n",
    "\n",
    "### 1.0. Issues of DDPG\n",
    "\n",
    "- Q-function overestimates Q-values dramatically\n",
    "- policy exploits the errors in the Q-function\\\n",
    "--> policy breaking\n",
    "\n",
    "### 1.1 Clipped double Q-Learning\n",
    "- TD3 learns two Q-functions instead of one (hence “twin”)\n",
    "- takes smaller Q-value to form the targets in the Bellman error loss functions\n",
    "- original wording: IF target networks can be used to reduce the error over multiple updates, and policy updates on high-error states cause divergent behavior, then the policy network should be updated at a lower frequencey than the value network, to first minimize error before introducing a policy update. We propose delaying policy updates until the value error is as small as possible. [...] The less frequent policy updates that do occur will use a value estimate with lower variance, and in principle, should result in higher quality policy updates. -> generates a \"two-timescale\" algorithmen.\n",
    "\n",
    "### 1.2. Delayed policy update\n",
    "- TD3 updates the policy (and target networks) less frequently than the Q-function\n",
    "- paper recommends one policy update for every two Q-function updates\n",
    "\n",
    "### 1.3. Target policy smoothing\n",
    "- Q-funktion sometimes has sharp spikes for several actions -> bad behaviour for policy\n",
    "- TD3 adds noise to the target action\n",
    "- DDPG adds noise only in output action\n",
    "- Q-Funktion smoothed by similar actions caused by added noise to target action $a'(s')$\n",
    "- harder for the policy to exploit Q-function errors by smoothing out Q along changes in action\n",
    "\n",
    "$$ a'(s') = clip(r + \\mu_{\\theta_{targ}}(s')+clip(\\epsilon,-c,c),a_{Low},a_{High}), \\epsilon{\\sim}N(0,\\sigma) $$\n",
    "\n",
    "#### Interesting Info\n",
    "- noise used for exploration in training\n",
    "- higher-quality training data by reducing the scale of the noise of target action over the course of training is an option\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "source: https://spinningup.openai.com/en/latest/algorithms/td3.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435b29a9",
   "metadata": {},
   "source": [
    "- - - \n",
    "\n",
    "## **2. Our TD3 implementation**\n",
    "\n",
    "\n",
    "\n",
    "### 2.1. RL algorithmus extension\n",
    "\n",
    "### 2.2. Hyperparameters\n",
    "\n",
    "### 2.3. Training\n",
    "\n",
    "### 2.4. Enjoy \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e1e92a",
   "metadata": {},
   "source": [
    "- - -\n",
    "\n",
    "## **3. Implementation tests**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5640405b",
   "metadata": {},
   "source": [
    "- - - \n",
    "\n",
    "## **4. Benchmark - Discussion**\n",
    "\n",
    "### 4.1. TD3 vs DDPG\n",
    "\n",
    "### 4.2. Our TD3 vs Stable Baselines\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721fc5f6",
   "metadata": {},
   "source": [
    "- - -\n",
    "\n",
    "## **5. Summary**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921a5517",
   "metadata": {},
   "source": [
    "- - -\n",
    "\n",
    "## **6. Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46f7913",
   "metadata": {},
   "source": [
    "- - -\n",
    "\n",
    "## **7. Further material**\n",
    "\n",
    "**GitHub Repos**\n",
    "- TD3-RLE: \n",
    "- Stable Baselines: \n",
    "https://github.com/openai/baselines\n",
    "DLR - RL Baselines Zoo: \n",
    "https://github.com/DLR-RM/rl-baselines3-zoo\n",
    "- OpenAI Spinning Up: https://spinningup.openai.com/en/latest/\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "438b8d175c626dc66ab93e1d243ebbf49c1b7127dc5ebb75e5a50a3421acac2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
