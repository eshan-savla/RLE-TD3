{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "31283171-0274-4a4e-9a04-527cc6079400",
   "metadata": {},
   "source": [
    "# Difference DDPG and TD3\n",
    "TD3 is a direct successor of DDPG and improves it using three major tricks.\n",
    "\n",
    "## Problems of DDPG\n",
    "- Q-function overestimates Q-values dramatically\n",
    "- policy exploits the errors in the Q-function\\\n",
    "--> policy breaking\n",
    "\n",
    "## 1. Clipped double Q-Learning\n",
    "- TD3 learns two Q-functions instead of one (hence “twin”)\n",
    "- takes smaller Q-value to form the targets in the Bellman error loss functions\n",
    "- original wording: IF target networks can be used to reduce the error over multiple updates, and policy updates on high-error states cause divergent behavior, then the policy network should be updated at a lower frequencey than the value network, to first minimize error before introducing a policy update. We propose delaying policy updates until the value error is as small as possible. [...] The less frequent policy updates that do occur will use a value estimate with lower variance, and in principle, should result in higher quality policy updates. -> generates a \"two-timescale\" algorithmen.\n",
    "\n",
    "## 2. Delayed policy update\n",
    "- TD3 updates the policy (and target networks) less frequently than the Q-function\n",
    "- paper recommends one policy update for every two Q-function updates\n",
    "\n",
    "## 3. Target policy smoothing\n",
    "- Q-funktion sometimes has sharp spikes for several actions -> bad behaviour for policy\n",
    "- TD3 adds noise to the target action\n",
    "- DDPG adds noise only in output action\n",
    "- Q-Funktion smoothed by similar actions caused by added noise to target action $a'(s')$\n",
    "- harder for the policy to exploit Q-function errors by smoothing out Q along changes in action\n",
    "\n",
    "$$ a'(s') = clip(r + \\mu_{\\theta_{targ}}(s')+clip(\\epsilon,-c,c),a_{Low},a_{High}), \\epsilon{\\sim}N(0,\\sigma) $$\n",
    "\n",
    "# Interesting Info\n",
    "- noise used for exploration in training\n",
    "- higher-quality training data by reducing the scale of the noise of target action over the course of training is an option\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "source: https://spinningup.openai.com/en/latest/algorithms/td3.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "438b8d175c626dc66ab93e1d243ebbf49c1b7127dc5ebb75e5a50a3421acac2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
