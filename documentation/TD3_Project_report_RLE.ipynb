{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "727b6ac3",
   "metadata": {},
   "source": [
    "# Twin-Delayed Deep Deterministic Policy Gradients\n",
    "\n",
    "Authors:\n",
    "\n",
    "- Eshan Savla    (Mtr-Nr. 91543)\n",
    "- Raphael Aberle (Mtr-Nr. 91386)\n",
    "- Leo Schäfer    (Mtr-Nr. 91430)\n",
    "\n",
    "<br>\n",
    "\n",
    "**Table of contents**:\n",
    "\n",
    "1. Introduction\n",
    "2. The path from DDPG to TD3 <br>\n",
    "   2.1. Motivation of TD3 <br>\n",
    "   2.2. Major differences bettween DDPG and TD3 <br>\n",
    "      &emsp; 2.2.1. Clipped double Q-Learning <br>\n",
    "      &emsp; 2.2.2. Target policy smooting <br>\n",
    "      &emsp; 2.2.3. Delayed policy update <br>\n",
    "      &emsp; 2.2.4. Overview\n",
    "3. Our TD3 implementation <br>\n",
    "   3.1. Extension of TD3 algorithm <br>\n",
    "   3.2. Hyperparameters <br>\n",
    "4. Test environment - Ant-v3\n",
    "5. Usage of RL TD3 algorithm <br>\n",
    "   5.1. Training phase<br>\n",
    "   5.2. Enjoy phase<br>\n",
    "   5.3. Evaluation\n",
    "6. Benchmark <br>\n",
    "      6.1. Untrained models <br>\n",
    "      6.2. Ground Truth <br>\n",
    "      &emsp; 6.2.1. Training phase <br>\n",
    "      &emsp; 6.2.2. Trained models <br>\n",
    "      6.3. TD3 Trained models - Hyperparameter tests <br>\n",
    "      6.4. Our TD3 Best vs. Stable Baselines <br>\n",
    "      &emsp; 6.4.1. Training phase <br>\n",
    "      &emsp; 6.4.2. Trained models <br>\n",
    "7. Summary \n",
    "8. Further material\n",
    "\n",
    "<br> \n",
    "<p>\n",
    "\n",
    "- - - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce86643",
   "metadata": {},
   "source": [
    "## **1. Introduction**\n",
    "This notebook is part of our final project submission of the project \"Twin-Delayed Deep Deterministic Policy Gradients\" in the course \"Robot Programming\" (RKIM121) from Prof. Dr.-Ing. Björn Hein and Gergely Sóti at the University of Applied Sciences in Karlsruhe.\n",
    "\n",
    "It aims to provide a comprehensive understanding of TD3 and its implementation. <br>\n",
    "The notebook includes explanations of the algorithm, code snippets, and discussions on benchmarks and performance.\n",
    "\n",
    "Please note that this notebook assumes familiarity with reinforcement learning concepts and algorithms. <br> \n",
    "If you are new to RL, it is recommended to first study the basics of DDPG before diving into TD3.\n",
    "\n",
    "&rarr; Let's get started!\n",
    "<br>\n",
    "\n",
    "- - - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31283171-0274-4a4e-9a04-527cc6079400",
   "metadata": {},
   "source": [
    "## **2. The path from DDPG to TD3**\n",
    "\n",
    "The TD3-Algorithm introduces several enhancements while being a direct successor to the DDPG algorithm. <br>\n",
    "Therefore, this chapter provides an overview of the motivations behind these extensions in the TD3 algorithm and points out the major differences between the two algorithms.\n",
    "\n",
    "\n",
    "### 2.1. Motivation of TD3\n",
    "\n",
    "Deep Deterministic Policy Gradient (DPPG) deals with the folowing issues: \n",
    "- **Overestimation Bias:** \n",
    "    - DDPG tends to overestimate the Q-Values from the Q-Value function with it's one critic network \n",
    "    - The Q-Value approximator sometimes develops an incorrect sharp peak for some actions\n",
    "    - The policy will quickly exploit that peak leading to brittle and incorect behaviour\n",
    "- High **sensitivity** to hyperparameters: \n",
    "    - DDPG reacts strongly to changes in hyperparameters\n",
    "- **Exploration is limited**: \n",
    "    - Exploration is inherently difficult for the DDPG Agent\n",
    "- **Limited robustness** during learning phase: \n",
    "    - The learning is relatively unstable. \n",
    "    - It requires larger sample sizes from the replay buffer to achieve more stability.\n",
    "\n",
    "### 2.2. Major differences bettween DDPG and TD3\n",
    "\n",
    "The fundamental tasks of TD3 are to minimize the overestimation of Q-Values and to generate more stable learning behavior. <br> \n",
    "To achieve this, three modifications are proposed for TD3.\n",
    "\n",
    "#### 2.2.1. Clipped double Q-Learning\n",
    "- TD3 learns two Q-Value functions instead of one (hence “twin”).\n",
    "- The smaller Q-Value from the targets is used in the Bellman error loss functions.\n",
    "\n",
    "$$ y(r,s',d) = r + \\gamma(1 - d) \\min_{i =  1,2} (Q{\\phi_{i,targ}}(s',a'(s'))) $$\n",
    "\n",
    "#### 2.2.2. Target policy smoothing\n",
    "- The Target policy smoothing combats the exploitation of errors from the Q-Value approximator.\n",
    "- Clipped noise is added to the target action $a'(s')$ during the policy update process.\n",
    "\n",
    "$$ a'(s') = clip(\\mu_{\\theta_{targ}}(s')+clip(\\epsilon,-c,c),a_{Low},a_{High}), \\epsilon{\\sim}N(0,\\sigma) $$\n",
    "\n",
    "- DDPG adds noise only to output action.\n",
    "- This noise addition makes it harder for the policy to exploit Q-function errors by smoothing out Q along changes in action.\n",
    "- It results in a more robust and stable learning behavior.\n",
    "- For a higher-quality training, reducing the scale of the noise of the target action over the course of training is an option.\n",
    "\n",
    "#### 2.2.3. Delayed policy update\n",
    "- TD3 updates the policy (and target networks) less frequently than the Q-Value function leading to an increase in stability.\n",
    "- The original paper of the TD3-Alogrithm recommends one policy (and target) update for every two Q-Value function updates.\n",
    "\n",
    "#### 2.2.4. Overview\n",
    "\n",
    "The resulting structure of the TD3 algorithm can be visually summarized with the following graphic: <br> <br>\n",
    "<img src=\"./images/td3_overview.png\" alt=\"td3_overview\" width=\"1000\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9fb471",
   "metadata": {},
   "source": [
    "- - - \n",
    "## **3. Our TD3 implementation**\n",
    "\n",
    "For a better structure, the algorithm implementations are seperated into multiple classes and files. The mentioned classes and files are also attached to this notebook. <br>\n",
    "For more detailed explanations of the whole code, please check the comments of the source code files.\n",
    "\n",
    "### 3.1. Extension of TD3 algorithm\n",
    "\n",
    "For a high-level overview, it is possible to review a short class description and the pseudocode below.\n",
    "\n",
    "**Class descriptions:**\n",
    "| Class              | Description |\n",
    "|---                 |---          |\n",
    "| Actor              | Contains the actor network for policy approximation  |\n",
    "| Critic             | Contains the critic network for Q-Value approximation |\n",
    "| Noise              | Creates noise with the Ornstein-Uhlenbeck process |\n",
    "| ReplayBuffer       | Contains the replay buffer for experience collection in Q-Learning |\n",
    "| DDPG               | Contains the DDPG-Agent. Defines how the DDPG-Agent acts and learns |\n",
    "| TD3                | Contains the TD3-Agent. Defines how the TD3-Agent acts and learns |\n",
    "\n",
    "**Pseudocode:** <br>\n",
    "\n",
    "The shown pseudocode gives an overview of the fundamental changes made for TD3 compared to DDPG. <br>\n",
    "\n",
    "<img src=\"./images/pseudocode.jpg\" alt=\"td3_overview\" width=\"500\"/>\n",
    "\n",
    "**Specific changes made for TD3-Agent** <br>\n",
    "\n",
    "- The below provided code snippets illustrate the enhancements incorporated into the TD3 implementation. For clarity, each TD3 method that differs from the DDPG-Algorithm is examined individually.\n",
    "- The three significant modifications in the TD3 implementation, as compared to the DDPG implementation, are distinctly color-coded for each extension.\n",
    "- The code presented primarily focuses only on the crucial transitions from DDPG to TD3.\n",
    "<br>\n",
    "\n",
    "For a more comprehensive understanding, please refer to the source code. \n",
    "   - <span style=\"color:Tomato\">**Clipped double Q-Learning**</span>\n",
    "   - <span style=\"color:MediumSeaGreen\">**Target policy smoothing**</span>\n",
    "   - <span style=\"color:DodgerBlue\">**Delayed policy update**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72770b85",
   "metadata": {},
   "source": [
    "<span style=\"color:MediumSeaGreen\">Noise</span> is incorporated into the target action to smooth the Q-value function, thereby mitigating the potential for exploiting errors in the Q-value function. <br>\n",
    "Training quality is enhanced by reducing the scale of noise.  <br>\n",
    "To counteract overestimation, the <span style=\"color:Tomato\">minimum of the value approximated by the two target Q-value functions</span>  is chosen for the computation of critic loss.<br><br>\n",
    "<img src=\"./images/compute_q_target_method.png\" alt=\"init_method\" width=\"1200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715761d6",
   "metadata": {},
   "source": [
    "The actor network and the target networks are only <span style=\"color:DodgerBlue\">updated every n-step</span> defined by the policy_freq parameter. <br><br>\n",
    "<img src=\"./images/learn_method.png\" alt=\"init_method\" width=\"750\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7628296",
   "metadata": {},
   "source": [
    "**Further remarks:**\n",
    "\n",
    "- Action space fix\n",
    "  - We've changed the implementation of the action space to accept values of multiple dimensions as required by the MuJoCo \"Ant-v3\" environment. <br>\n",
    "  - Neglecting this, we were only able to train one leg. \n",
    "  - For reference, please review line 168 in td3.py.\n",
    "<br>\n",
    "- Training fix\n",
    "  - Instead of using the done parameter, we are using the parameters \"terminated\" and \"truncated\" to stop a training episode. \n",
    "  - Consequently, \"terminated\" and \"truncated\" are mapped to the done parameter with a logical \"or\" connection.\n",
    "  - Neglecting this, our episode durations had exceeded 1000 timesteps and would corrupt the results.\n",
    "  - For reference, please review training sections in main_td3.py and the function compute_avg_return in functions.py.\n",
    "<br>\n",
    "- Training budget usage \n",
    "  - Instead of using epochs and training steps, we've changed the implementation to use training steps only. \n",
    "  - Neglecting this, an early stop of an epoch (e.g. earlier than 1000 steps) would lead to a nearlier overall training stop (e.g. earlier than 1 million timesteps).\n",
    "  - Consequently, it enables us to use the complete training budget.\n",
    "  - For reference, please review training sections in main_td3.py.\n",
    "<br>\n",
    "- Delayed training start\n",
    "  - By integrating a delayed learning start (e.g. starting at 10.000 timesteps), we've optimized the training process as the first experiences are not exploited directly. \n",
    "  - Instead, we are collecting first experiences to stabilize the training convergence.\n",
    "  - For reference, please review training sections in main_td3.py.   \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435b29a9",
   "metadata": {},
   "source": [
    "### 3.2. Hyperparameters\n",
    "\n",
    "The hyperparameters are one way of influencing the algorithm and model quality. \n",
    "\n",
    "**General parameters:​**\n",
    "- Discount factor (γ) -> immediate rewards > future rewards​\n",
    "  - value range: 0 - 1 => higher values emphasize long-term rewards\n",
    "- Learning-rate (lr)  -> learning rate for the neural network optimizer\n",
    "  - Determines the size of steps taken​ during the optimization process during updates\n",
    "- ε-greedy (ε)        -> choose a random action with ε-probability for exploration​\n",
    "- Update weight (τ)   -> how much target network is updated\n",
    "  - Determines the rate of updating the target networks. It is slowly to reduce the variance in learning and prevent overly influence of most recent experiences.\n",
    "  \n",
    "**TD3 specific parameters:​**\n",
    "- Target action noise (σ) -> smoothing the q-function by adding noise to action​\n",
    "  - It combats overfitting and prevents the learning process to become too reliant on current policy estimates.\n",
    "- Update frequency        -> Update rate of actor and target networks​\n",
    "  - Usually, the policy and target networks get updated every second update of the critic network.\n",
    "\n",
    "<br>\n",
    "During the implementation of TD3, we've integrated further parameters to improve the model results. \n",
    "\n",
    "\n",
    "**Further parameters:​**\n",
    "- Noise reduction -> reduce noise over ascending time steps​\n",
    "  - The noise gets reduces over ascending timesteps in a e-function.\n",
    "- Min noise factor   -> max noise reduction\n",
    "  - Introduced to maintain a minimum noise within higher time steps. \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df473d6e",
   "metadata": {},
   "source": [
    "## **4. Test environment - Ant-v3**\n",
    "\n",
    "To perform a proper benchmark of different algorithms, an environment is required. <p>\n",
    "\n",
    "Consequently, this chapter describes the used test environment \"Ant-v3\" from MuJoCo.<br>\n",
    "\n",
    "MuJoCo stands for Multi Joint dynamics with Contact. <br>\n",
    "It is a physics engine designed for simulating and controlling the dynamics of rigid body systems. <p>\n",
    "\n",
    "<img src=\"./images/ant-v3.gif\" alt=\"ant-v3\" width=\"300\"/>\n",
    "\n",
    "\n",
    "**- Action Space​**\n",
    "  - An ant with 4 legs\n",
    "  - 2 joints in each leg sum up to 8​ joints in total\n",
    "  - It is possible to apply a continuous torque from -1 to 1 Nm​ to each joint.\n",
    "<br>\n",
    "\n",
    "**- Observation Space​**\n",
    "  - 27 – 29 (extended) observations​\n",
    "    - Positions of the torso (x, y, z)\n",
    "    - velocities (in x-, y-, z-direction) \n",
    "    - angles (around x-, y-, z-axis)\n",
    "    - angular velocities ​(around the x-, y-, z-axis)\n",
    "\n",
    "**- Reward​**\n",
    "  - An important element in Reinforcement Learning is the reward definition.\n",
    "  - In Ant-v3, the reward is defined as:\n",
    "     $$ reward = healthy reward + forward reward - ctrl cost - contact cost​ $$\n",
    "\n",
    "     - healthy_reward: Is received every timesteps the torso is within the healthy_z_range (default: [0,2:1]) \n",
    "     - forward_reward: Motion in x-direcetion is rewarded   \n",
    "         - aim: as fast as possible\n",
    "     - ctrl_cost: Negative reward is received if the taken actions are to large \n",
    "         - ctrl_cost_weight * sum(action²)  (default = 0.5)\n",
    "     - contact_cost: Negative reward is received if the external contact force is too large (jumping etc.) \n",
    "         - contact_cost_weight * sum(clip(external contact force to contact_force_range)²)​\n",
    "\n",
    "​\n",
    "**- Episode end​**\n",
    "  - Any state space value is no longer finite​\n",
    "  - Truncation: The z-coordinate of the torso is not in the defined interval (default: [0,2:1])\n",
    "  - Termination: The episode duration reaches 1000 timesteps\n",
    "\n",
    "\n",
    "For further informations, please review: https://www.gymlibrary.dev/environments/mujoco/ant/\n",
    "\n",
    "<br>\n",
    "\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b87fa70",
   "metadata": {},
   "source": [
    "## 5. Usage of RL TD3 algorithm \n",
    "\n",
    "Disclaimer: <p>\n",
    "We've experienced that the code execution in the notebook is quite slow and could lead to problems with MuJoCo.<br>\n",
    "For better performance, please execute the files directly from the repository. <br>\n",
    "Furthermore, the user experience can be improved by using a gpu as our comparison has shown that the training is 7% faster. (the explicit deviation is hardware dependent) <br>\n",
    "\n",
    "<p>\n",
    "<p>\n",
    "\n",
    "### 5.1. Training phase\n",
    "For a better overview of hyperparameter sets used for training, the hydra library is used. <br>\n",
    "It facilitates the loading of hyperparameter sets from different config files.<br>\n",
    "A new config file for changing the hyperparameters could be created by copying one of the existing ones in the config folder and adjust it's values.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f3420a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "from hydra.utils import instantiate\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.insert(0, '../src') # DO NOT CHANGE\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from td3 import TD3Agent\n",
    "from replay_buffer import ReplayBuffer\n",
    "from functions import compute_avg_return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778ba210",
   "metadata": {},
   "source": [
    "The to be used config file for training can be specified like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d8c93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_name = \"config\"\n",
    "with hydra.initialize(config_path=\"../configs/\", job_name=\"td3_config\"):\n",
    "    cfg = hydra.compose(config_name=config_name) # Option to test multiple configurations: Change the config name to your desired config file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe4de80",
   "metadata": {},
   "source": [
    "Below, you will find the main function for the training agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9e863f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_training(load_replay_buffer:bool = True):\n",
    "    \"\"\"\n",
    "    The main function for running the TD3 training algorithm.\n",
    "\n",
    "    Parameters:\n",
    "        - load_replay_buffer (bool): Whether to load the replay buffer from a checkpoint. Default is True.\n",
    "\n",
    "    Returns:\n",
    "        - None\n",
    "    \"\"\"\n",
    "    physical_devices = tf.config.list_physical_devices('GPU') \n",
    "    for device in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(device, True)\n",
    "    replay_buffer = instantiate(cfg.ReplayBuffer)\n",
    "    env = gym.make('Ant-v3', max_episode_steps=1000,autoreset=True, render_mode='rgb_array') #human \n",
    "    agent = TD3Agent(env.action_space, env.observation_space.shape[0],gamma=cfg.TD3Agent.gamma,tau=cfg.TD3Agent.tau, epsilon=cfg.TD3Agent.epsilon, noise_clip=cfg.TD3Agent.noise_clip, policy_freq=cfg.TD3Agent.policy_freq)\n",
    "    if type(cfg.TD3Agent.use_checkpoint_timestamp) == bool and cfg.TD3Agent.use_checkpoint_timestamp:\n",
    "        print(\"Loading most recent checkpoint\")\n",
    "        agent.load_weights(use_latest=True)\n",
    "        if load_replay_buffer:\n",
    "            replay_buffer.load(agent.save_dir)\n",
    "    elif not cfg.TD3Agent.use_checkpoint_timestamp:\n",
    "        print(\"No checkpoint loaded. Starting from scratch.\")\n",
    "    else:\n",
    "        print(\"Loading weights from timestamp: \", cfg.TD3Agent.use_checkpoint_timestamp)\n",
    "        agent.load_weights(load_dir=os.path.join(cfg.TD3Agent.weights_path, cfg.TD3Agent.use_checkpoint_timestamp+'/'), use_latest=False)\n",
    "        if load_replay_buffer:\n",
    "            replay_buffer.load(agent.save_dir)\n",
    "    total_timesteps = cfg.Training.start\n",
    "    returns = list()\n",
    "    actor_losses = list()\n",
    "    critic1_losses = list() \n",
    "    critic2_losses = list()\n",
    "    evals_dir = None\n",
    "    first_training = True\n",
    "    eval_count = 0\n",
    "    with tqdm(total=cfg.Training.timesteps, desc=\"Timesteps\", leave=True) as pbar:\n",
    "        while total_timesteps <= cfg.Training.timesteps:\n",
    "            obs, _ = env.reset()\n",
    "            # gather experience\n",
    "            agent.noise_output_net.reset()\n",
    "            agent.noise_target_net.reset()\n",
    "\n",
    "            ep_actor_loss = 0\n",
    "            ep_critic1_loss = 0\n",
    "            ep_critic2_loss = 0\n",
    "            steps = 0\n",
    "            for j in range(1000):\n",
    "                steps += 1\n",
    "                action = agent.act(np.array([obs]), random_action=(total_timesteps < cfg.Training.start_learning)) # i < 1 no policy for first episode\n",
    "                # execute action\n",
    "\n",
    "                # Patching terminated/truncated state behaviour based on issue:\n",
    "                # https://github.com/Farama-Foundation/Gymnasium/pull/101\n",
    "                # and\n",
    "                # https://github.com/openai/gym/issues/3102\n",
    "                new_obs, r, terminated, truncated, info = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                if steps >= 1000:\n",
    "                    episode_truncated = not done or info.get(\"TimeLimit.truncated\", False)\n",
    "                    info[\"TimeLimit.truncated\"] = episode_truncated\n",
    "                    # truncated may have been set by the env too\n",
    "                    truncated = truncated or episode_truncated\n",
    "                    done = terminated or truncated\n",
    "                replay_buffer.put(obs, action, r, new_obs, done)\n",
    "                obs = new_obs\n",
    "                if done:\n",
    "                    break\n",
    "               \n",
    "            total_timesteps += steps\n",
    "            \n",
    "            if total_timesteps >= cfg.Training.start_learning:      \n",
    "            # Learn from the experiences in the replay buffer.\n",
    "                for s in range(cfg.Training.batch_size):\n",
    "                    s_states, s_actions, s_rewards, s_next_states, s_dones = replay_buffer.sample(cfg.Training.sample_size, cfg.Training.unbalance)\n",
    "                    actor_l, critic1_l, critic2_l = agent.learn(s_states, s_actions, s_rewards, s_next_states, s_dones,s, total_timesteps)\n",
    "                    ep_actor_loss += actor_l\n",
    "                    ep_critic1_loss += critic1_l\n",
    "                    ep_critic2_loss += critic2_l\n",
    "                if eval_count % 25 == 0 or first_training:\n",
    "                    first_training = False\n",
    "                    avg_return, _, _, _, _ = compute_avg_return(env, agent, num_episodes=5, max_steps=1000, render=False)\n",
    "                    print(\n",
    "                        f'Timestep {total_timesteps}, actor loss {ep_actor_loss / steps}, critic 1 loss {ep_critic1_loss / steps}, critic 2 loss {ep_critic2_loss/steps} , avg return {avg_return}')\n",
    "                    agent.save_weights()\n",
    "                    replay_buffer.save(agent.save_dir)\n",
    "                if evals_dir is None:\n",
    "                    evals_dir = '../evals/'+ agent.save_dir.split('/')[-2] + \"/\"\n",
    "                    os.makedirs(evals_dir, exist_ok=True)   # create folder if not existing yet\n",
    "                returns.append(avg_return)\n",
    "                actor_losses.append(tf.get_static_value(ep_actor_loss) / steps)\n",
    "                critic1_losses.append(tf.get_static_value(ep_critic1_loss) / steps)\n",
    "                critic2_losses.append(tf.get_static_value(ep_critic2_loss) / steps)\n",
    "                df = pd.DataFrame({'returns': returns, 'actor_losses': actor_losses, 'critic1_losses': critic1_losses, 'critic2_losses': critic2_losses})\n",
    "                plot_losses = df.drop(\"returns\", axis=1, inplace=False).plot(title='TD3 losses', figsize=(10, 5))\n",
    "                plot_losses.set(xlabel='Epochs', ylabel='Loss')\n",
    "                plot_losses.get_figure().savefig(evals_dir+'losses_td3.png')\n",
    "\n",
    "                returns_df = pd.DataFrame({'returns': returns})\n",
    "                plot_returns = returns_df.plot(title='TD3 returns', figsize=(10, 5))\n",
    "                plot_returns.set(xlabel='Epochs', ylabel='Returns')\n",
    "                plot_returns.get_figure().savefig(evals_dir+'returns_td3.png')\n",
    "                plt.close('all')\n",
    "                df.to_csv(evals_dir+'td3_results.csv', index=True) \n",
    "                eval_count += 1\n",
    "            pbar.update(steps)\n",
    "\n",
    "\n",
    "    agent.save_weights()\n",
    "    replay_buffer.save(agent.save_dir)\n",
    "    \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca74fe9",
   "metadata": {},
   "source": [
    "In oder execute the training with the current loaded config, please run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d11adf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed_time = timeit.timeit(lambda: main_training(load_replay_buffer=True), number=1)\n",
    "minutes, seconds = divmod(elapsed_time, 60)\n",
    "print(f\"The main function ran for {int(minutes)} minutes and {seconds:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6e666d",
   "metadata": {},
   "source": [
    "\n",
    "### 5.2. Enjoy phase\n",
    "\n",
    "After training or after loading a pretrained models, the agent can be evaluated by the main_enjoy function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301dd9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddpg import DDPGAgent\n",
    "from td3 import TD3Agent\n",
    "import os\n",
    "import gymnasium as gym\n",
    "from functions import compute_avg_return\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0be8663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_enjoy(agent_type:str, load_dir:str=None, use_latest:str=True, render:bool=True, num_episodes:int=150):  #defaults: agent_type=\"td3\", load_dir=None, use_latest=True, render_mode=None\n",
    "    \"\"\"\n",
    "    This function allows you to enjoy a trained agent in the environment.\n",
    "\n",
    "    Parameters:\n",
    "        agent_type (str): Specify the agent type you want to enjoy. Options: \"ddpg\" or \"td3\"\n",
    "        load_dir (str, optional): Defaults to None.\n",
    "        use_latest (str, optional): Defaults to True.\n",
    "        render (bool, optional): True  => render the environment visually // False => Run enjoy without environement and agent rendering. Defaults to True.\n",
    "    Returns:\n",
    "        - None\n",
    "    \"\"\"\n",
    "    if load_dir is not None:\n",
    "        use_latest = False\n",
    "    os.chdir(os.path.dirname(os.path.abspath(__file__)))                    # change directory to the directory of this file\n",
    "    render_mode = \"human\" if render else \"rgb_array\"\n",
    "    env = gym.make(id='Ant-v3', autoreset=True, render_mode = render_mode)  # create the environment \n",
    "                                                                                # id = Environment ID \n",
    "                                                                                # autoreset=True => automatically reset the environment after an episode is done\n",
    "                                                                                #render_mode='human' => render the environment visually // render_mode='rgb_array' => render the environment as an array to collect results\n",
    "    if agent_type == \"ddpg\":   #if you want to enjoy a DDPG agent\n",
    "        from ddpg_config import cfg as ddpg_cfg, config_name\n",
    "        #create a DDPGAgent with the same parameters as the one used for training and configurations specified in yaml file (loaded via Hydra)\n",
    "        agent = DDPGAgent(env.action_space, env.observation_space.shape[0],gamma=ddpg_cfg.DDPGAgent.gamma,tau=ddpg_cfg.DDPGAgent.tau, epsilon=0) \n",
    "    \n",
    "    elif agent_type == \"td3\":  #if you want to enjoy a TD3 agent\n",
    "        #create a TD3Agent with the same parameters as the one used for training and configurations specified in yaml file (loaded via Hydra)\n",
    "        from td3_config import cfg as td3_cfg, config_name\n",
    "        agent = TD3Agent(env.action_space, env.observation_space.shape[0],gamma=td3_cfg.TD3Agent.gamma,tau=td3_cfg.TD3Agent.tau, epsilon=0)\n",
    "    else: #handling wrong agent type specification\n",
    "        raise ValueError(\"Invalid agent type\")\n",
    "    \n",
    "    agent.load_weights(load_dir=load_dir, use_latest=use_latest) #load the weights of the agent\n",
    "    obs, _ = env.reset() #reset the environment and get the initial observation\n",
    "    \n",
    "    avg_return, avg_return_stddev, episode_no, returns, stddevs = compute_avg_return(env, agent, num_episodes=num_episodes, max_steps=1000, render=render) #compte the average return and specify the to be evaluated number of episodes\n",
    "    print(f\"Average return: {avg_return}, Standard deviation: {avg_return_stddev}\")\n",
    "    \n",
    "    #To get a unique benchmark result, we save the results in a csv file\n",
    "    time_stamp = agent.save_dir.split(\"/\")[-2]  #get timestamp \n",
    "    user_name = os.getlogin()                   #get username \n",
    "    df = pd.DataFrame({\"file\": [config_name], \"time_stamp\": [time_stamp], \"user_name\": [user_name], \"agent_type\": [agent_type], \"avg_return\": [avg_return], \"return_stddev\": [avg_return_stddev], \"episode_no\": [episode_no], \"returns\": [returns], \"stddevs\": [stddevs]}) #create pandas DF\n",
    "    df.to_csv(\"../benchmarks_new.csv\", mode=\"a\", header=False, index=False)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae7d6a4",
   "metadata": {},
   "source": [
    "Execute the enjoy here, by specifying the following parameters: <br>\n",
    "- the training algorithm\n",
    "- the directory of the trained model for the evaluation\n",
    "- a flag, whether the function should just use the latest trained model (stored in your folder with the models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890bd159",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_enjoy(\"td3\", render=False)  #speficy the algorithm/ agent type (\"ddpg\" or \"td3\"), specify the path to the model you want to evaluate. For more information: see Mapping_mod-conf.md,  True if you want to use the latest checkpoint     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d9463d",
   "metadata": {},
   "source": [
    "### 5.3 Evaluation\n",
    "\n",
    "For evaluation purposes, we save average returns and losses in cvs files while training.  <br>\n",
    "For the benachmark tests the return in each episode and the standard deviation of the returns is saved in a csv file. <br> \n",
    "\n",
    "Both files can be evaluated by using the evaluate.py file:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ae6272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "\n",
    "from functions import flatten\n",
    "import datetime\n",
    "import numpy as np\n",
    "import datetime\n",
    "from scipy import stats\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce4be4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_enjoy(data_path_csv:str, plot_type: str = 'bar', plot_avgs:bool = False, plot_timeseries:bool = False, **kwargs):\n",
    "    \"\"\"\n",
    "    This function allows you to evaluate the enjoy phase of a trained model based on the benchmark results of the different agents.\n",
    "\n",
    "    Parameters:\n",
    "        data_path_csv (str, optional): _description_. Specify the path to the csv file\n",
    "        plot_type (str, optional): _description_. Specify the plot_type for the graph. Default = 'bar'.\n",
    "        only_avgs (bool, optional): _description_. Default = False.\n",
    "        plot_title (str, optional): _description_. Specify plot tile. Default = None\n",
    "        x_axis_title (str, optional): _description_. Specify title of x axis. Default = None\n",
    "        y_axis_title (str, optional): _description_. Specify title of y axis. Default = None\n",
    "    Returns:\n",
    "        - None\n",
    "    \"\"\"\n",
    "    plot_title = kwargs.get(\"plot_title\", \"Returns per Episode for different configurations\")\n",
    "    x_axis_title = kwargs.get(\"x_axis_title\", \"Episode\")\n",
    "    y_axis_title = kwargs.get(\"y_axis_title\", \"Returns per Episode\")\n",
    "    # Read data from CSV file\n",
    "    data = pd.read_csv(data_path_csv)\n",
    "    \n",
    "    # Convert the string representation of lists into actual lists of floats\n",
    "    data['returns'] = data['returns'].apply(ast.literal_eval)\n",
    "    data['stddevs'] = data['stddevs'].apply(ast.literal_eval)\n",
    "    data['episode_no'] = data['episode_no'].apply(ast.literal_eval)\n",
    "        \n",
    "    if isinstance(data['episode_no'], str):\n",
    "        episode_no = ast.literal_eval(data['episode_no'])\n",
    "\n",
    "    data['stddevs'] = data['stddevs'].apply(flatten)\n",
    "\n",
    "    # Calculate the mean of the lists in 'stddevs' column\n",
    "    data['stddevs'] = data['stddevs'].apply(np.mean)\n",
    "\n",
    "    # Group the data and calculate the mean standard deviation per episode over all experiments\n",
    "    mean_stddev_per_episode = data.groupby(['time_stamp', 'config_name', 'user_name', 'agent_type'])['stddevs'].mean()\n",
    "\n",
    "    # Update y1 and y2 values\n",
    "    data['y1'] = data.apply(lambda row: np.mean(row['returns']) + mean_stddev_per_episode[(row['time_stamp'], row['config_name'], row['user_name'], row['agent_type'])], axis=1)\n",
    "    data['y2'] = data.apply(lambda row: np.mean(row['returns']) - mean_stddev_per_episode[(row['time_stamp'], row['config_name'], row['user_name'], row['agent_type'])], axis=1)\n",
    "\n",
    "    # Create a linegraph with x-axis = episode, y-axis=returns per episode\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "    if plot_type.lower() == 'line':\n",
    "        fig, ax = plt.subplots()\n",
    "        for i, row in data.iterrows():\n",
    "            episode_no = row['episode_no']\n",
    "            returns = row['returns']\n",
    "            stddevs = row['stddevs']\n",
    "            \n",
    "            # Convert the list into a string representation\n",
    "            episode_no_str = str(episode_no)\n",
    "            # Convert the string representation of episode numbers into actual list of floats\n",
    "            episode_no = ast.literal_eval(episode_no_str)\n",
    "            \n",
    "            # Plot the returns per episode\n",
    "            label = f' {row[\"agent_type\"]}-{row[\"config_name\"]}'\n",
    "            label_means = f'{row[\"agent_type\"]}-{row[\"config_name\"]}-mean'\n",
    "            \n",
    "            if plot_timeseries:\n",
    "                ax.plot(episode_no, returns, label=label)\n",
    "            if plot_avgs:\n",
    "                ax.plot(episode_no, [row[\"avg_return\"] for i in range(len(returns))], label=label_means)\n",
    "            \n",
    "            # Add a subplot using fill_between\n",
    "            if plot_timeseries:\n",
    "                ax.fill_between(episode_no, np.array(returns) + np.array(stddevs), np.array(returns) - np.array(stddevs), alpha=0.3)\n",
    "            if plot_avgs:\n",
    "                ax.fill_between(episode_no, np.array([row[\"avg_return\"] for i in range(len(returns))]) + np.array([row[\"avg_return_stddev\"] for i in range(len(returns))]), np.array([row[\"avg_return\"] for i in range(len(returns))]) - np.array([row[\"avg_return_stddev\"] for i in range(len(returns))]), alpha=0.3)\n",
    "\n",
    "        # Set the labels and title and legend\n",
    "        ax.set_xlabel(x_axis_title)\n",
    "        ax.set_ylabel(y_axis_title)\n",
    "        ax.set_title(plot_title)\n",
    "        ax.legend()\n",
    "                \n",
    "        # Save the plot with the unique timestamp in the file name\n",
    "        fig.savefig(f'{timestamp}_returns_per_episode.png')\n",
    "\n",
    "    if plot_type.lower() == 'bar':\n",
    "        # Create a barchart with x-axis = configuration, y-axis=average return\n",
    "        color=['green', 'blue', 'blue', 'gold', 'gold', 'darkred',  'darkred', 'darkgreen', 'darkorchid', 'darkorchid', 'cadetblue']\n",
    "        X = data['config_name']\n",
    "        Y = data['avg_return']\n",
    "        t = type(Y)\n",
    "        yerr = data['avg_return_stddev']\n",
    "        \n",
    "        fig, ax = plt.subplots() #create figure and axes\n",
    "        #set the size of the figure\n",
    "        fig.set_figheight(10)\n",
    "        fig.set_figwidth(25)\n",
    "        ax.bar(X, Y, yerr=yerr, align='center', color=color[0:len(X)], ecolor='black', capsize=10)\n",
    "        #set the labels (including ticks and it's parameter) and title\n",
    "        ax.set_title(plot_title, fontsize=25)\n",
    "        ax.set_ylabel(y_axis_title, fontsize=25)\n",
    "        ax.set_xlabel(x_axis_title, fontsize=25)\n",
    "        ax.set_xticklabels(X, rotation=45, fontsize=30)\n",
    "        ax.tick_params(axis='y', labelsize=30)\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(f'{timestamp}_avg_return_per_config.png') #save the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab982387",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_training(training_data_path_csv, **kwargs):\n",
    "    \"\"\"\n",
    "    Evaluate the training by plotting the actor and critic losses over time.\n",
    "\n",
    "    Parameters:\n",
    "        training_data_path_csv (str): The file path to the training data in CSV format.\n",
    "        plot_title (str, optional): The title of the plot. Default is None. Actor or critic will be inserted to title automatically\n",
    "        x_axis_title (str, optional): The title of the x-axis. Default is None.\n",
    "        y_axis_title (str, optional): The title of the y-axis. Default is None.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    plot_title = kwargs.get(\"plot_title\", \"Losses over Time\")\n",
    "    x_axis_title = kwargs.get(\"x_axis_title\", \"Timesteps x 1e5\")\n",
    "    y_axis_title = kwargs.get(\"y_axis_title\", \"Loss\")\n",
    "    # Load the training data from the CSV file\n",
    "    training_data = pd.read_csv(training_data_path_csv)\n",
    "\n",
    "    plt.figure(figsize=(15,10))     #specify figure size\n",
    "\n",
    "    # Plot the actor loss over time\n",
    "    X = np.array(list(range(1000, 1000001, int(1000000/len(training_data['actor_losses']))))[-len(training_data['actor_losses']):])\n",
    "    plt.plot(X, training_data['actor_losses'], color = \"blue\",label='Actor Loss')\n",
    "    \n",
    "    # Set labels and title\n",
    "    plt.xlabel(x_axis_title, fontsize=30)\n",
    "    plt.ylabel(y_axis_title, fontsize=30)\n",
    "    #plt.title('Actor Loss over Time', fontsize=30)\n",
    "    \n",
    "    # Visualize a trend line\n",
    "    x = training_data['Unnamed: 0']\n",
    "    y = training_data['actor_losses']\n",
    "    z = np.polyfit(X, y, 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(X, p(X), \"r--\", label='Trend Line', linewidth=6)\n",
    "    plt.xticks(range(0,1000000, 100000), fontsize=30) #set ticks for x-axis labels\n",
    "    plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0)) #format the labels in scientific notation\n",
    "    plt.tick_params(axis='y', labelsize=30) #set ticks for y-axis labels and labelsize\n",
    "\n",
    "    # Add legend\n",
    "    plt.legend(fontsize=30)\n",
    "    plt.title('Actor ' + plot_title, fontsize=30)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save plot\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\") #create unigque timestamp\n",
    "    filename = f\"{training_data_path_csv}_{timestamp}_actor_losses.png\"\n",
    "    plt.savefig(filename)\n",
    "\n",
    "    # Plot of critic loss\n",
    "    plt.figure(figsize=(15,10)) #define figure size\n",
    "\n",
    "    # Plot the critics loss of critic 1 over time\n",
    "    x1 = training_data['Unnamed: 0']\n",
    "    y1 = training_data['critic1_losses']\n",
    "    plt.plot(X, y1, label='Critic loss', color='red')\n",
    "    plt.xticks(range(0,1000000, 100000), fontsize=30) #set ticks for x-axis labels\n",
    "    plt.tick_params(axis='y', labelsize=30) #set ticks for y-axis labels and labelsize\n",
    "    plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0)) #format the labels in scientific notation\n",
    "\n",
    "    # Plot the critics loss of critic 2 over time\n",
    "    x2 = training_data['Unnamed: 0']\n",
    "    y2 = training_data['critic2_losses']\n",
    "    plt.plot(X, y2, label='Critic loss', color='#1f77ba')\n",
    "    plt.xticks(range(0,1000000, 100000), fontsize=30)\n",
    "    plt.tick_params(axis='y', labelsize=30)\n",
    "    plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "\n",
    "    # Calculate and plot trend line for critics losses\n",
    "    # Critic 2\n",
    "    slope2, intercept2, _, _, _ = stats.linregress(X, y2)\n",
    "    plt.plot(X, intercept2 + slope2*X, 'r--', label='Trend line 2', linewidth=6)\n",
    "    plt.xticks(range(0,1000000, 100000), fontsize=30)\n",
    "    plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "    plt.tick_params(axis='y', labelsize=30)\n",
    "\n",
    "    # Critic 1\n",
    "    slope1, intercept1, _, _, _ = stats.linregress(X, y1)\n",
    "    plt.plot(X, intercept1 + X*slope1, 'r--', label='Trend line 1', linewidth=6)\n",
    "    plt.xticks(range(0,1000000, 100000), fontsize=30)\n",
    "    plt.tick_params(axis='y', labelsize=30)\n",
    "    plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "    \n",
    "    # Set labels and title\n",
    "    plt.xlabel(x_axis_title, fontsize=30)\n",
    "    plt.ylabel(y_axis_title, fontsize=30)\n",
    "    plt.title('Critic ' + plot_title, fontsize=30)\n",
    "\n",
    "    # Add a legend\n",
    "    plt.legend(fontsize=30)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot\n",
    "    plt.savefig(f\"{training_data_path_csv}_{timestamp}_critic_losses.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd970e18",
   "metadata": {},
   "source": [
    "Execute the following code to view the training evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022f7f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the csv file to evaluate the training results\n",
    "training_data_path_csv = '../models/td3_best/td3_results.csv'\n",
    "evaluate_training(training_data_path_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec8fa50",
   "metadata": {},
   "source": [
    "Execute the following code to review the enjoy evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8a8601",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the path to the csv file to evaluate the benchmark results\n",
    "data_path_csv = '../benchmarks/benchmarks_td3_test_hp.csv'\n",
    "evaluate_enjoy(data_path_csv=data_path_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5640405b",
   "metadata": {},
   "source": [
    "- - -\n",
    "## **6. Benchmark**\n",
    "\n",
    "This chapter deals with the development of our TD3 implementation driven by various parameter tests. <br>\n",
    "Furthermore, we will benchmark the implementation to untrained models, basic ground truth models and stable baseline´s model. <br>\n",
    "\n",
    "#### 6.1. Untrained models\n",
    "In order to evaluate the performance of our models, their performance was compared to that of untrained ddpg and td3 agents. <br>\n",
    "The following graph shows how these untrained agents perform in the same environment by carrying out entirely random actions along all joints.\n",
    "\n",
    "<img src=\"images/returns_untrained.png\" width=\"500\">\n",
    "\n",
    "<p>\n",
    "\n",
    "| Untrained returns  | DDPG | TD3 |\n",
    "|---                 |---   |---  |\n",
    "| average return     | 773  | 761 |\n",
    "| standard deviation | 167  | 198 |\n",
    "\n",
    "</p>\n",
    "\n",
    "### 6.2. Ground Truth\n",
    "\n",
    "To compare the two algorithms, we've defined a basic parameter set, the so-called Ground Truth. \n",
    "\n",
    "The below shown graphic gives an overview to the Ground Truth parameters: \n",
    "\n",
    "<img src=\"images/ground_truth_parameterset.png\" width=\"500\"> \n",
    "\n",
    "\n",
    "#### 6.2.1. Training phase\n",
    "\n",
    "This chapter compares the training/ learning phase of the ground truth models from DDPG and TD3.\n",
    "\n",
    "**Actor losses over time:** <p>\n",
    "\n",
    "| DDPG | TD3 |\n",
    "| ---- | --- |\n",
    "| <img src=\"images/ddpg_results.csv_2024-01-15-20-42_actor_losses.png\" width=\"400\"> | <img src=\"images/td3_results.csv_2024-01-15-20-40_actor_losses.png\" width=\"400\"> |\n",
    "</p>\n",
    "\n",
    "Investigating the visualizations shows the following key insights:\n",
    "- Delayed policy updates lead to more stable actor training losses.​\n",
    "  - This gets visible by the comparison of the success and precessor actor losses of the two algorithms. \n",
    "  - The relative difference is smaller in TD3 than in DDPG. \n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "**Critics losses over time:** <p>\n",
    "| DDPG | TD3 |\n",
    "| ---- | --- |\n",
    "| <img src=\"images/ddpg_results.csv_2024-01-15-20-42_critic_losses.png\" width=\"400\"> | <img src=\"images/td3_results.csv_2024-01-15-20-40_critic_losses.png\" width=\"400\"> |\n",
    "</p>\n",
    "\n",
    "Investigating the visualizations shows the following key insights:\n",
    "\n",
    "- Double Q learning combats overestimation successfully \n",
    "  - This gets visible in the critic training losses of TD3. By using the minimum of the two critic losses, the overestimation is tackled. \n",
    "  - The arithmetic average difference is 0.92% leading to a successful improve. ​\n",
    "- Added target policy noise smoothens exploitation of a possible q-function error ​\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "#### 6.2.2. Trained models\n",
    "\n",
    "After a complete training process, you can observe the trained models interacting in the environment as e.g. shown in the videos below: <p>\n",
    "| DDPG | TD3 |\n",
    "| ---- | --- |\n",
    "|<img src=\"images/ddpg_gt.gif\" width=\"400\"> | <img src=\"images/td3_gt.gif\" width=\"400\">|\n",
    "</p>\n",
    "\n",
    "\n",
    "In order to quantify the quality of the trained models regarding performance, the average returns and standard deviations of the episodes can be used.\n",
    "\n",
    "Our benchmarked uses a budget of 150,000 steps and evaluates 150 episodes (with max. 1.000 steps each) in total.\n",
    "\n",
    "The results of this benchmark can be visualized as shown below: <br>\n",
    "\n",
    "<img src=\"images/ddpg_gt_td3_gt.png\" width = 500>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "|                    | DDPG | TD3 |\n",
    "|---                 |---   |---  |\n",
    "| average return     |  170 | 631 |\n",
    "| standard deviation |  259 | 470 |\n",
    "\n",
    "\n",
    "Consequently, the applied algorithm changes lead to a remarkably increased performance in the average returns by a factor of ~3,7 and reduced standard deviation ratio to average return by factor of ~2. <br>\n",
    "\n",
    "<p>\n",
    "\n",
    "Considering only the empirical implications of the results from the untrained agents, suggests that carrying out entirely random actions seems to be more profitable than attempting to traverse the environment along its x axis as fast as possible. \n",
    "\n",
    "While the untrained models aren't able to walk like the ground truth models, they seem to be able to exploit the rewards system to gain higher rewards.\n",
    "\n",
    "This suggests that suboptimal hyperparemeters were chosen and paves the way for more detailed hyperparameters tests in order to improve the performance of our models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f6dd57",
   "metadata": {},
   "source": [
    "\n",
    "### 6.3. TD3 Trained models - Hyperparameter tests \n",
    "\n",
    "Compared to the ground truth, various isolated permutations of the hyperparameters have shown the following result: \n",
    "\n",
    "<img src=\"images/2024-01-16_00-12_avg_return_per_config.png\" width=800>\n",
    "\n",
    "A review of the graph suggests that an isolated increase of tau and of the learning rate have the biggest influence. <br>\n",
    "\n",
    "As the influence of the learning is higher than the one of tau, the following evaluation concentrates on models with a learning rate of 0.001. <br>\n",
    "\n",
    "Additional permutations of the noise have show that a combination of a higher learning rate and a higher noise lead to a remarkable increase in the average reward of a trained model. Increasing the tau value in this situation led to lower average returns.\n",
    "\n",
    "Building on this, the policy noise was reduced using a negative exponential function over time to provide adequate noise towards the initial and medial stages while ensuring a rapid drop in noise towards the final stages of training. <br> \n",
    "\n",
    "This form of noise reduction led to drastic improvements in the average returns.\n",
    "\n",
    "\n",
    "### 6.4. Our TD3 Best vs. Stable Baselines\n",
    "\n",
    "This section deals with the benchmark of our best TD3 compared to the one of stable baselines. <p>\n",
    "\n",
    "Stable baselines is a set of high-quality implementation of reinforcement learning algorithms in python. <br> \n",
    "It is built on top of OpenAI Gym containing the Ant-v3 environment as well.<p>\n",
    "\n",
    "For more information, please check the following link: https://stable-baselines.readthedocs.io/en/master/\n",
    "\n",
    "#### 6.4.1. Training phase \n",
    "\n",
    "By using our best TD3 parameter set, we can observe the following training/ learning process. \n",
    "\n",
    "<img src=\"images/td3_best_actor_losses.png\" width=\"400\"> <img src=\"images/td3_best_critic_losses.png\" width=\"400\">\n",
    "\n",
    "Compared to the ground truth, a similar trend in the losses was observed. <br> \n",
    "It can be noted, that loss reduction for the actor was higher than that of the ground truths. <br>\n",
    " Even the trendline of the critic losses displays a steeper gradient. \n",
    "\n",
    "<p>\n",
    "\n",
    "#### 6.4.2. Trained models\n",
    "\n",
    "After a complete training process, you can observe the trained models interacting in the environment as e.g. shown in the videos below: \n",
    "| TD3 Best | TD3 Stable baselines |\n",
    "| ---- | --- |\n",
    "| <img src=\"images/td3_best.gif\" width=\"400\"> | <img src=\"images/sb3.gif\" width=\"400\"> |\n",
    "\n",
    "\n",
    "In order to quantify the quality of the trained models, the average return and standard deviation of an episode can be used.\n",
    "\n",
    "Our benchmarked uses a budget of maximum 150,000 steps and evaluates 150 episodes (with max. 1.000 steps each) in total.\n",
    "\n",
    "The result of this benchmark can be visualized as shown below: <br>\n",
    "\n",
    "<img src=\"images/td3_baselines_best_gt_ddpg_gt.png\" width = 500>\n",
    "\n",
    "<p>\n",
    "\n",
    "\n",
    "| Model                | Average return | Standard deviation |\n",
    "|---                   |---             |---                 |\n",
    "| DDPG Ground Truth    |     170        |      259           |\n",
    "| TD3 Ground Truth     |     631        |      470           |\n",
    "| TD3 Best model       |     5354       |      767           |\n",
    "| TD3 Stable Baselines |     5813       |      589           |\n",
    "\n",
    "<p>\n",
    "A comparision of the performance of our best model with that from stable baselines shows, that our average returns lays behind by a margin of less than 500. <br>\n",
    "\n",
    "\n",
    "**Further remarks:** \n",
    "- Reward shaping of the healthy z-range (increasing max. possible  healthy z-coordinate of the torso ) would lead to increased returns ​and decreased standard deviations.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721fc5f6",
   "metadata": {},
   "source": [
    "- - -\n",
    "\n",
    "## **7. Summary**\n",
    "\n",
    "<br>- In general, TD3 achieved better results than DDPG through simple changes​.\n",
    "- The change of the training budget implementation (training steps only) enabled a complete usage of the training budget.\n",
    "- Overall, a reduction in the TD3 training time (compared to DDPG) was noticed. \n",
    "- While individual components contribute little to improvement(Fujimoto, et al. (2018))​, the combination of all three proved to be a performance accelarator.\n",
    "<br/>\n",
    "<br>\n",
    "- Without any training, the untrained TD3 and DDPG model performed better than our ground truth trained models.\n",
    "- Subsequent parameter tests proved that our ground truth parameters had been suboptimal.\n",
    "- Thereby, a higher learning rate combined with target policy action noise and reduction over time greatly improved results.\n",
    "- The result of our best TD3 model is slightly lower than the one of stable baselines. Taking the standard deviation in consideration, the perfomance of our best TD3 model is comparable to stable baselines.\n",
    "<br/>\n",
    "<br>\n",
    "Future potential:\n",
    "- Further in depth studies for TD3 parameter permutations could yield even better results​.\n",
    "- More in depth studies for parameter permutations of DDPG could confirm sensitivity reduction​ as observed in other studies.\n",
    "- Our implementation could benefit from an optimization for replay buffer and parallel training to accelerate the training process and decrease the hardware dependency.\n",
    "<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46f7913",
   "metadata": {},
   "source": [
    "- - -\n",
    "\n",
    "## **8. Further material**\n",
    "\n",
    "Additional information can be found via the below mentioned links.\n",
    "\n",
    "**GitHub Repos:**\n",
    "- TD3-RLE: https://github.com/eshan-savla/RLE-TD3.git\n",
    "- Stable Baselines: https://github.com/openai/baselines\n",
    "- DLR - RL Baselines Zoo: https://github.com/DLR-RM/rl-baselines3-zoo\n",
    "\n",
    "**Environment from MuJoCo:**\n",
    "- Ant-v3: https://www.gymlibrary.dev/environments/mujoco/ant/\n",
    "\n",
    "**Useful information:**\n",
    "- OpenAI Spinning Up: https://spinningup.openai.com/en/latest/  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "438b8d175c626dc66ab93e1d243ebbf49c1b7127dc5ebb75e5a50a3421acac2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
