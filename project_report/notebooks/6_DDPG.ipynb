{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2f002a4",
   "metadata": {},
   "source": [
    "# Original given implementation for DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97861abd",
   "metadata": {},
   "source": [
    "This notebook was the given implementation of DDPG as a basis for our project in the course \"robot programming\" at the University of Applied Sciences in Karlsruhe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd16655",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc2fcf66-a275-479e-8d6e-20baac59d8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-21 14:55:06.711055: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-21 14:55:07.120867: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-21 14:55:07.121086: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-21 14:55:07.171271: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-21 14:55:07.291561: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-21 14:55:10.111690: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/eshan/.local/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from collections import deque\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31283171-0274-4a4e-9a04-527cc6079400",
   "metadata": {},
   "source": [
    "# Deep Deterministic Policy Gradient\n",
    "\n",
    "DDPG is an Actor-Critic RL algorithm. It has a policy approximation model and a value approximation model. Its policy network produces continuous actions, and combines them with a DQN-like q-value estimator.\n",
    "\n",
    "DDPG uses 2 main models. The Actor network learns the policy and the Critic network learns the q-value. The Actor network receives the state as input and outputs an action vector, corresponding to the continuous action space (e.g. joint velocities), thus it produces a deterministic policy. The Critic receives the output of the Actor, combines it with the state, and approximates a q-value. \n",
    "\n",
    "Since the learned the policy is deterministic, we need to implement some kind of exploration during the training process. To make DDPG policies explore better, a noise to their actions. Originally an Ornstein-Uhlenbeck (https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.) noise was proposed, however it is often used with normal-distributed noise or parameter noise, directly applied to the network parameters.\n",
    "\n",
    "## Agent\n",
    "\n",
    "Similarly to the PolicyGradient agent, the DDPG agent also has \"frozen\" models. Both the Actor and the Critic network have their target Actor and target Critic counterparts.\n",
    "\n",
    "In case of the Critic, the learning step is almost identical to the learning step of the DQN. The only difference is, that here, the actions are directly fed into the network, as input, and the output is the corresponding q-value, instead of having multiple q-values for each possible action.\n",
    "\n",
    "The interesting part is the training of the Actor. Since the goal is to maximize the q-value with the policy, and we already have a differentiable q-value approximator, the solution is sort of obvious. We just feed the output of the Actor $\\pi_\\phi$ into our Critic $c_\\theta$, along with the state, negate the result and use it as loss.\n",
    "\n",
    "$$\n",
    "loss_{actor} = -c_\\Theta (s_t, \\pi_\\Phi (s_t))\n",
    "$$\n",
    "\n",
    "If the Critic is doing its job well, and the Actor minimizes the negated output of the Critic, then it should maximize a well approximated q-value, thus also approaching a well working policy.\n",
    "\n",
    "## Replay buffer\n",
    "\n",
    "Similar to the case of the DQN, a replay buffer is used to store gathered experience, and later sample from it during the training phase. In this case however, we adjust the samplint method to faver resent experiences over older experiences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8bc290f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 400\n",
      "1 300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, u in enumerate((400,300)):\n",
    "    print(i,u)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cda979ca-9d0f-495f-8870-29cee1b60464",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(tf.keras.layers.Layer):\n",
    "    def __init__(self, units=(400, 300), n_actions=2, **kwargs):\n",
    "        super(Actor, self).__init__(**kwargs)\n",
    "        self.layers = []\n",
    "        for i, u in enumerate(units):\n",
    "            self.layers.append(tf.keras.layers.Dense(u, activation=tf.nn.leaky_relu,\n",
    "                                                     kernel_initializer=tf.keras.initializers.glorot_normal())) # Zwei layers mit neuronen\n",
    "        last_init = tf.random_normal_initializer(stddev=0.0005)\n",
    "        self.layers.append(tf.keras.layers.Dense(n_actions, activation='tanh', kernel_initializer=last_init)) # letzte Layer welches die Aktionen ausgibt\n",
    "\n",
    "    def call(self, inputs, **kwargs): # forward pass\n",
    "        outputs = inputs\n",
    "        for l in self.layers:\n",
    "            outputs = l(outputs)\n",
    "        return outputs\n",
    "    \n",
    "class Critic(tf.keras.layers.Layer):\n",
    "    def __init__(self, state_units=(400, 300), action_units=(300,), units=(150,), **kwargs):\n",
    "        super(Critic, self).__init__(**kwargs)\n",
    "        self.layers_state = []\n",
    "        for u in state_units:\n",
    "            self.layers_state.append(tf.keras.layers.Dense(u, activation=tf.nn.leaky_relu,\n",
    "                                                           kernel_initializer=tf.keras.initializers.glorot_normal())) # Layers für die States\n",
    "\n",
    "        self.layers_action = []\n",
    "        for u in action_units:\n",
    "            self.layers_action.append(tf.keras.layers.Dense(u, activation=tf.nn.leaky_relu,\n",
    "                                                            kernel_initializer=tf.keras.initializers.glorot_normal()))  # Layers für die Actions\n",
    "\n",
    "        self.layers = []\n",
    "        for u in units:\n",
    "            self.layers.append(tf.keras.layers.Dense(u, activation=tf.nn.leaky_relu,\n",
    "                                                     kernel_initializer=tf.keras.initializers.glorot_normal())) # Layers für die Q-Werte\n",
    "        last_init = tf.random_normal_initializer(stddev=0.00005) \n",
    "        self.layers.append(tf.keras.layers.Dense(1, kernel_initializer=last_init)) # letzte Layer welche den Q-Wert ausgibt\n",
    "\n",
    "        self.add = tf.keras.layers.Add()\n",
    "         \n",
    "    def call(self, inputs, **kwargs):\n",
    "        p_action = inputs['action']\n",
    "        p_state = inputs['state']\n",
    "\n",
    "        for l in self.layers_action:\n",
    "            p_action = l(p_action) # forward pass für die Actions\n",
    "\n",
    "        for l in self.layers_state:\n",
    "            p_state = l(p_state) # forward pass für die States\n",
    "\n",
    "        outputs = self.add([p_state, p_action])\n",
    "        for l in self.layers:\n",
    "            outputs = l(outputs) # forward pass für die Q-Werte\n",
    "\n",
    "        return outputs # Q-Wert\n",
    "    \n",
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
    "        x = (\n",
    "                self.x_prev\n",
    "                + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "                + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        # Store x into x_prev\n",
    "        # Makes next noise dependent on current one\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)\n",
    "            \n",
    "class DDPGAgent:\n",
    "    def __init__(self, action_space, observation_shape, gamma=0.99, tau=0.001, epsilon=0.05):\n",
    "        self.action_space = action_space\n",
    "        self.tau = tau  # target network weight adaptation\n",
    "        self.gamma = gamma  # discount factor\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.actor = Actor(n_actions=action_space.shape[0]) # Actor und Critic initialisieren\n",
    "        self.critic = Critic()\n",
    "\n",
    "        self.target_actor = Actor(n_actions=action_space.shape[0]) # Target Actor und Critic initialisieren\n",
    "        self.target_critic = Critic()\n",
    "\n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001) # Optimizer für Actor und Critic\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "        self.noise = OUActionNoise(mean=np.zeros(np.array(self.action_space.sample()).shape),\n",
    "                                   std_deviation=float(0.2) * np.ones(1)) # Noise für die Exploration\n",
    "\n",
    "        self._init_networks(observation_shape)\n",
    "\n",
    "    def _init_networks(self, observation_shape):\n",
    "        initial_state = np.zeros([1, observation_shape])\n",
    "\n",
    "        initial_action = self.actor(initial_state) # Forward pass mit initialen Werten\n",
    "        self.target_actor(initial_state)\n",
    "\n",
    "        critic_input = {'action': initial_action, 'state': initial_state}\n",
    "        self.critic(critic_input) # Forward pass mit initialen Werten\n",
    "        self.target_critic(critic_input)\n",
    "\n",
    "        self.target_actor.set_weights(self.actor.get_weights()) # Target Actor und Critic mit den initialen Werten initialisieren\n",
    "        self.target_critic.set_weights(self.critic.get_weights())\n",
    "\n",
    "    @staticmethod\n",
    "    def update_target(model_target, model_ref, tau=0.0):\n",
    "        new_weights = [tau * ref_weight + (1 - tau) * target_weight for (target_weight, ref_weight) in\n",
    "                       list(zip(model_target.get_weights(), model_ref.get_weights()))] # zip macht aus zwei Listen eine Liste von Tupeln wo Elemente der gleichen Indizes zusammengefasst werden\n",
    "        model_target.set_weights(new_weights)\n",
    "\n",
    "    def act(self, observation, explore=True, random_action=False):\n",
    "        if random_action or np.random.uniform(0, 1) < self.epsilon:\n",
    "            a = self.action_space.sample() # explore with random action\n",
    "        else:\n",
    "            a = self.actor(observation).numpy()[:, 0] # sample action from policy\n",
    "            if explore:\n",
    "                a += self.noise() # add noise for exploration\n",
    "        a = np.clip(a, self.action_space.low, self.action_space.high) # setzt alle Wert größer als high auf high und alle kleiner als low auf low\n",
    "        return a\n",
    "\n",
    "    def compute_target_q(self, rewards, next_states, dones):\n",
    "        actions = self.target_actor(next_states) # forward pass mit den nächsten States gibt die nächsten Actions gemäß der Policy aus target_actor\n",
    "        critic_input = {'action': actions, 'state': next_states} \n",
    "        next_q = self.target_critic(critic_input) # forward pass mit den nächsten Actions und States gibt die nächsten Q-Werte aus target_critic\n",
    "        target_q = rewards + (1 - dones) * next_q * self.gamma \n",
    "        return target_q \n",
    "\n",
    "    def get_actor_grads(self, states):\n",
    "        with tf.GradientTape() as tape: # GradientTape speichert alle Operationen die auf Variablen ausgeführt werden\n",
    "            actions = self.actor(states) # forward pass mit den States gibt die Actions gemäß der Policy aus actor nicht target_actor\n",
    "            critic_input = {'action': actions, 'state': states}\n",
    "            qs = self.critic(critic_input) # forward pass mit den Actions und States gibt die Q-Werte aus critic nicht target_critic\n",
    "            loss = -tf.math.reduce_mean(qs) # loss ist der negative Durchschnitt der Q-Werte. Ziel ist loss möglichst zu minimieren. Heißt negativere Zahl ist besser.\n",
    "        gradients = tape.gradient(loss, self.actor.trainable_variables) # Gradienten berechnen, die loss nach den Gewichten und Biases ableiten. Partielle Ableitung\n",
    "        gradients = [tf.clip_by_value(grad, -1.0, 1.0) for grad in gradients] # Gradienten clippen um zu verhindern, dass die Gradienten zu groß werden\n",
    "        return gradients, loss\n",
    "\n",
    "    def get_critic_grads(self, states, actions, target_qs):\n",
    "        with tf.GradientTape() as tape: \n",
    "            critic_input = {'action': actions, 'state': states} # kommt aus Replay Buffer\n",
    "            qs = self.critic(critic_input) # forward pass mit den Actions und States gibt die Q-Werte aus critic nicht target_critic\n",
    "            loss = tf.reduce_mean(tf.abs(target_qs - qs)) # loss ist der Durchschnitt der absoluten Differenz zwischen den Q-Werten und den target Q-Werten. \n",
    "        gradients = tape.gradient(loss, self.critic.trainable_variables) # Partielle Ableitung\n",
    "        gradients = [tf.clip_by_value(grad, -1.0, 1.0) for grad in gradients]\n",
    "        return gradients, loss\n",
    "\n",
    "    def learn(self, states, actions, rewards, next_states, dones):\n",
    "        target_qs = self.compute_target_q(rewards, next_states, dones) # target Q-Werte berechnen aus Replay Buffer\n",
    "\n",
    "        actor_grads, actor_loss = self.get_actor_grads(states)\n",
    "        critic_grads, critic_loss = self.get_critic_grads(states, actions, target_qs)\n",
    "\n",
    "        self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables)) # Gewichte und Biases updaten anhand der Gradienten\n",
    "        self.critic_optimizer.apply_gradients(zip(critic_grads, self.critic.trainable_variables))\n",
    "        self.target_update() # target Actor und Critic updaten\n",
    "        return actor_loss, critic_loss\n",
    "\n",
    "    def target_update(self):\n",
    "        DDPGAgent.update_target(self.target_critic, self.critic, self.tau)\n",
    "        DDPGAgent.update_target(self.target_actor, self.actor, self.tau)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.p_indices = [0.5 / 2] # Prioritized Experience Replay\n",
    "\n",
    "    def put(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append([state, action, np.expand_dims(reward, -1), next_state, np.expand_dims(done, -1)]) \n",
    "\n",
    "    def sample(self, batch_size=1, unbalance=0.8):\n",
    "        p_indices = None\n",
    "        if random.random() < unbalance:\n",
    "            self.p_indices.extend((np.arange(len(self.buffer) - len(self.p_indices)) + 1)\n",
    "                                  * 0.5 + self.p_indices[-1]) \n",
    "            p_indices = self.p_indices / np.sum(self.p_indices) # Priorisierung der Buffer Elemente neu berechnen\n",
    "        sample_idx = np.random.choice(len(self.buffer),\n",
    "                                      size=min(batch_size, len(self.buffer)),\n",
    "                                      replace=False,\n",
    "                                      p=p_indices) # Priorisierung neuer Elemente im Buffer\n",
    "        sample = [self.buffer[s_i] for s_i in sample_idx]\n",
    "        states, actions, rewards, next_states, dones = map(np.array, zip(*sample)) # states, actions, rewards, next_states, dones aus dem Buffer holen\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f03d5208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25, 0.75, 1.25, 1.75, 2.25]\n",
      "[0.04 0.12 0.2  0.28 0.36]\n"
     ]
    }
   ],
   "source": [
    "p_indices = [0.5 / 2]\n",
    "p_indices.extend((np.arange(5 - len(p_indices)) + 1) * 0.5 + p_indices[-1])\n",
    "print(p_indices)    \n",
    "p_indices = p_indices / np.sum(p_indices)\n",
    "print(p_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2060c7-ad34-49be-b975-98bebb7cc60b",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The training loop is similar to the other training loops we saw until now. Except, for periodically updating and adapting the parameter noise, and we let the agent collect some initial experience in the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8b78f6a-5345-4ab1-aaac-8b2cf363a2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(env, agent, num_episodes=1, max_steps=200, render=False):\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        episode_return = 0.0\n",
    "        done = False\n",
    "        steps = 0\n",
    "        while not (done or steps > max_steps):\n",
    "            if render:\n",
    "                clear_output(wait=True)\n",
    "                plt.axis('off')\n",
    "                plt.imshow(env.render())\n",
    "                plt.show()\n",
    "            action = agent.act(np.array([obs]))\n",
    "            obs, r, done, _, _ = env.step(action)\n",
    "            episode_return += r\n",
    "            steps += 1\n",
    "        total_return += episode_return\n",
    "    return total_return / num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0952a592-1234-4e11-aba7-6c3545190bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(-2.0, 2.0, (1,), float32)\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "replay_buffer = ReplayBuffer()\n",
    "\n",
    "env = gym.make('Pendulum-v1', render_mode='rgb_array')\n",
    "print(env.action_space)\n",
    "print(env.action_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6a2cab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, actor loss 4.202342510223389, critic loss 1.1146107912063599 , avg return -1387.7986914836542\n",
      "epoch 25, actor loss 13.055190086364746, critic loss 0.12748092412948608 , avg return -1722.8429729657787\n",
      "epoch 50, actor loss 23.05086898803711, critic loss 0.24652034044265747 , avg return -1120.4033704239375\n",
      "epoch 75, actor loss 35.11097717285156, critic loss 0.33455321192741394 , avg return -914.5630150424963\n",
      "epoch 100, actor loss 35.81495666503906, critic loss 0.4388526976108551 , avg return -438.01075982693897\n",
      "epoch 125, actor loss 28.037904739379883, critic loss 0.32999247312545776 , avg return -623.671564501418\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m128\u001b[39m):\n\u001b[1;32m     22\u001b[0m     s_states, s_actions, s_rewards, s_next_states, s_dones \u001b[38;5;241m=\u001b[39m replay_buffer\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;241m64\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m     actor_l, critic_l \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms_rewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms_next_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms_dones\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     ep_actor_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m actor_l\n\u001b[1;32m     25\u001b[0m     ep_critic_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m critic_l\n",
      "Cell \u001b[0;32mIn[9], line 165\u001b[0m, in \u001b[0;36mDDPGAgent.learn\u001b[0;34m(self, states, actions, rewards, next_states, dones)\u001b[0m\n\u001b[1;32m    162\u001b[0m critic_grads, critic_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_critic_grads(states, actions, target_qs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_optimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(actor_grads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39mtrainable_variables)) \u001b[38;5;66;03m# Gewichte und Biases updaten anhand der Gradienten\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic_optimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcritic_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_update() \u001b[38;5;66;03m# target Actor und Critic updaten\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m actor_loss, critic_loss\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py:1223\u001b[0m, in \u001b[0;36mOptimizer.apply_gradients\u001b[0;34m(self, grads_and_vars, name, skip_gradients_aggregation, **kwargs)\u001b[0m\n\u001b[1;32m   1221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_gradients_aggregation \u001b[38;5;129;01mand\u001b[39;00m experimental_aggregate_gradients:\n\u001b[1;32m   1222\u001b[0m     grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggregate_gradients(grads_and_vars)\n\u001b[0;32m-> 1223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py:652\u001b[0m, in \u001b[0;36m_BaseOptimizer.apply_gradients\u001b[0;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_weight_decay(trainable_variables)\n\u001b[1;32m    651\u001b[0m grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(grads, trainable_variables))\n\u001b[0;32m--> 652\u001b[0m iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal_apply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;66;03m# Apply variable constraints after applying gradients.\u001b[39;00m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m variable \u001b[38;5;129;01min\u001b[39;00m trainable_variables:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py:1253\u001b[0m, in \u001b[0;36mOptimizer._internal_apply_gradients\u001b[0;34m(self, grads_and_vars)\u001b[0m\n\u001b[1;32m   1249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mesh \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_with_dtensor:\n\u001b[1;32m   1250\u001b[0m     \u001b[38;5;66;03m# Skip any usage of strategy logic for DTensor\u001b[39;00m\n\u001b[1;32m   1251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_internal_apply_gradients(grads_and_vars)\n\u001b[0;32m-> 1253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__internal__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaybe_merge_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1254\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distributed_apply_gradients_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1255\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distribution_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1257\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/distribute/merge_call_interim.py:51\u001b[0m, in \u001b[0;36mmaybe_merge_call\u001b[0;34m(fn, strategy, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Maybe invoke `fn` via `merge_call` which may or may not be fulfilled.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03mThe caller of this utility function requests to invoke `fn` via `merge_call`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03m  The return value of the `fn` call.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strategy_supports_no_merge_call():\n\u001b[0;32m---> 51\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m distribute_lib\u001b[38;5;241m.\u001b[39mget_replica_context()\u001b[38;5;241m.\u001b[39mmerge_call(\n\u001b[1;32m     54\u001b[0m       fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py:1345\u001b[0m, in \u001b[0;36mOptimizer._distributed_apply_gradients_fn\u001b[0;34m(self, distribution, grads_and_vars, **kwargs)\u001b[0m\n\u001b[1;32m   1342\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_step(grad, var)\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m grad, var \u001b[38;5;129;01min\u001b[39;00m grads_and_vars:\n\u001b[0;32m-> 1345\u001b[0m     \u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextended\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_grad_to_update_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m   1347\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_ema:\n\u001b[1;32m   1350\u001b[0m     _, var_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mgrads_and_vars)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:3013\u001b[0m, in \u001b[0;36mStrategyExtendedV2.update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   3010\u001b[0m   fn \u001b[38;5;241m=\u001b[39m autograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[1;32m   3011\u001b[0m       fn, autograph_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   3012\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy()\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m-> 3013\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3014\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3015\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replica_ctx_update(\n\u001b[1;32m   3016\u001b[0m       var, fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs, group\u001b[38;5;241m=\u001b[39mgroup)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:4083\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   4080\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update\u001b[39m(\u001b[38;5;28mself\u001b[39m, var, fn, args, kwargs, group):\n\u001b[1;32m   4081\u001b[0m   \u001b[38;5;66;03m# The implementations of _update() and _update_non_slot() are identical\u001b[39;00m\n\u001b[1;32m   4082\u001b[0m   \u001b[38;5;66;03m# except _update() passes `var` as the first argument to `fn()`.\u001b[39;00m\n\u001b[0;32m-> 4083\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_non_slot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:4089\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update_non_slot\u001b[0;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[1;32m   4085\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_non_slot\u001b[39m(\u001b[38;5;28mself\u001b[39m, colocate_with, fn, args, kwargs, should_group):\n\u001b[1;32m   4086\u001b[0m   \u001b[38;5;66;03m# TODO(josh11b): Figure out what we should be passing to UpdateContext()\u001b[39;00m\n\u001b[1;32m   4087\u001b[0m   \u001b[38;5;66;03m# once that value is used for something.\u001b[39;00m\n\u001b[1;32m   4088\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m UpdateContext(colocate_with):\n\u001b[0;32m-> 4089\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4090\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_group:\n\u001b[1;32m   4091\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:596\u001b[0m, in \u001b[0;36mcall_with_unspecified_conversion_status.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    595\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mControlStatusCtx(status\u001b[38;5;241m=\u001b[39mag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mUNSPECIFIED):\n\u001b[0;32m--> 596\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py:1342\u001b[0m, in \u001b[0;36mOptimizer._distributed_apply_gradients_fn.<locals>.apply_grad_to_update_var\u001b[0;34m(var, grad)\u001b[0m\n\u001b[1;32m   1340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_step_xla(grad, var, \u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_key(var)))\n\u001b[1;32m   1341\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1342\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py:241\u001b[0m, in \u001b[0;36m_BaseOptimizer._update_step\u001b[0;34m(self, gradient, variable)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_key(variable) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_dict:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m    234\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe optimizer cannot recognize variable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvariable\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis usually means you are trying to call the optimizer to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.keras.optimizers.legacy.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    240\u001b[0m     )\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/optimizers/adam.py:204\u001b[0m, in \u001b[0;36mAdam.update_step\u001b[0;34m(self, gradient, variable)\u001b[0m\n\u001b[1;32m    202\u001b[0m     v_hat\u001b[38;5;241m.\u001b[39massign(tf\u001b[38;5;241m.\u001b[39mmaximum(v_hat, v))\n\u001b[1;32m    203\u001b[0m     v \u001b[38;5;241m=\u001b[39m v_hat\n\u001b[0;32m--> 204\u001b[0m \u001b[43mvariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign_sub\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py:141\u001b[0m, in \u001b[0;36mweak_tensor_binary_op_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 141\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_auto_dtype_conversion_enabled\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    143\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "agent = DDPGAgent(env.action_space, env.observation_space.shape[0])\n",
    "for i in range(1001):\n",
    "    obs, _ = env.reset()\n",
    "    # gather experience\n",
    "    agent.noise.reset()\n",
    "    ep_actor_loss = 0\n",
    "    ep_critic_loss = 0\n",
    "    steps = 0\n",
    "    for j in range(200):\n",
    "        steps += 1\n",
    "        env.render()\n",
    "        action = agent.act(np.array([obs]), random_action=(i < 1)) # i < 1 weil bei ersten Epoche keine Policy vorhanden ist\n",
    "        # execute action\n",
    "        new_obs, r, done, _, _ = env.step(action)\n",
    "        replay_buffer.put(obs, action, r, new_obs, done)\n",
    "        obs = new_obs\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    # Learn from the experiences in the replay buffer.\n",
    "    for _ in range(128):\n",
    "        s_states, s_actions, s_rewards, s_next_states, s_dones = replay_buffer.sample(64)\n",
    "        actor_l, critic_l = agent.learn(s_states, s_actions, s_rewards, s_next_states, s_dones)\n",
    "        ep_actor_loss += actor_l\n",
    "        ep_critic_loss += critic_l\n",
    "        \n",
    "    if i % 25 == 0:\n",
    "        avg_return = compute_avg_return(env, agent, num_episodes=2, render=False)\n",
    "        print(\n",
    "            f'epoch {i}, actor loss {ep_actor_loss / steps}, critic loss {ep_critic_loss / steps} , avg return {avg_return}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c387af69-45de-4ff8-a7db-43b01eda44dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "compute_avg_return(env, agent, num_episodes=10, render=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0916dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
