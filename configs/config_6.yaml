# change from ground truth: target policy noise 0.2 -> 0.
ReplayBuffer:
  _target_: replay_buffer.ReplayBuffer
  capacity: 1000000

Training:
  start: 0
  epochs: 1001
  max_steps: 10000
  batch_size: 128
  sample_size: 256
  unbalance: 0.8
  eval_steps: 1000

Actor:
  units: [400, 300]
  stddev: 0.00005

Critic:
  state_units: [400, 300]
  action_units: [300,]
  units: [150,]
  stddev: 0.00005

OUNoiseOutput:
  _partial_: true
  theta: 0.15
  sigma: 0.2
  dt: 0.01

OUNoiseTarget:
  _partial_: true
  theta: 0.15
  sigma: 0.2
  dt: 0.01

DDPGAgent:
  gamma: 0.99
  tau: 0.005
  epsilon: 0.1
  learning_rate: 0.0003
  weights_path: ../checkpoints/

TD3Agent:
  gamma: 0.99
  tau: 0.005
  epsilon: 0.1
  learning_rate: 0.0003
  policy_freq: 2
  noise_clip: 0.5
  weights_path: ../checkpoints/
  use_checkpoint_timestamp: false # false -> fresh training; true -> continue training with latest checkpoint; timestamp -> continue training with checkpoint at timestamp

    
 

