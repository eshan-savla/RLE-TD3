# Change from ground truth: policy smoothing noise from 0.2 -> 0.5
ReplayBuffer:
  _target_: replay_buffer.ReplayBuffer
  capacity: 100000

Training:
  epochs: 1001
  max_steps: 1000
  batch_size: 128
  sample_size: 64
  unbalance: 0.8

Actor:
  units: [400, 300]
  stddev: 0.00005

Critic:
  state_units: [400, 300]
  action_units: [300,]
  units: [150,]
  stddev: 0.00005

OUNoise:
  _partial_: true
  theta: 0.15
  sigma: 0.2
  dt: 0.01

DDPGAgent:
  gamma: 0.99
  tau: 0.001
  epsilon: 0.1
  learning_rate: 0.0001

TD3Agent:
  gamma: 0.99
  tau: 0.005
  epsilon: 0.1
  learning_rate: 0.0003
  policy_freq: 2
  noise_clip: 0.5
  weights_path: ../checkpoints/
    
 

